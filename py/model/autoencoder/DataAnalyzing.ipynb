{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "564af787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from autoencoder import Autoencoder\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b8590",
   "metadata": {},
   "source": [
    "#### Constants and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3b4a3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running directory: /Users/igorvinokur/Development/Dev/Study/generative-model/py/model/autoencoder\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Running directory:\",os.getcwd())\n",
    "\n",
    "# CONSTS\n",
    "TEST_PURPOSE = False\n",
    "\n",
    "USE_PCA = True\n",
    "EPOCHS = 100\n",
    "scalers = {}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addfde0",
   "metadata": {},
   "source": [
    "#### Data processing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acb51838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(original_data):\n",
    "    norm_data = original_data\n",
    "    for i in original_data.columns:\n",
    "        scaler = None\n",
    "        if ('scaler_' + i) not in scalers:\n",
    "            scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        else:\n",
    "            scaler = scalers['scaler_' + i]\n",
    "        s_s = scaler.fit_transform(norm_data[i].values.reshape(-1, 1))\n",
    "        s_s = np.reshape(s_s, len(s_s))\n",
    "        scalers['scaler_' + i] = scaler\n",
    "        norm_data[i] = s_s\n",
    "    return norm_data\n",
    "\n",
    "def unnormalize_data(columns, reconstructed_data):\n",
    "    restored_data = []\n",
    "    for (i, name) in enumerate(columns):\n",
    "        restored_data.append(np.array(scalers['scaler_' + name].inverse_transform(reconstructed_data[:,i].reshape(-1, 1))).flatten())\n",
    "    return np.array(restored_data)\n",
    "\n",
    "\n",
    "def split_data(data, window):\n",
    "    \"\"\"\n",
    "    Data split for train and labels with sliding window\n",
    "    :param data: original data\n",
    "    :param window: subsequence length\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_x = data[:window + 1]\n",
    "    x, y = sliding_windows(train_x, 1)\n",
    "    train_x = Variable(torch.Tensor(np.array(x)))\n",
    "    train_y = Variable(torch.Tensor(np.array(y)))\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def sliding_windows(data, seq_length):\n",
    "    \"\"\"\n",
    "    Sliding window transformation for the data\n",
    "    :param data: original data\n",
    "    :param seq_length: window size\n",
    "    :return: two arrays of x and y\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)-seq_length):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b7659",
   "metadata": {},
   "source": [
    "#### LSTM Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ace7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, test_data, anomaly_norm_data, current_run_dir=None):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    objective = nn.MSELoss().to(device)\n",
    "    history = []\n",
    "    test_losses = []\n",
    "    anomaly_losses = []\n",
    "    for e in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        loss = 0\n",
    "        test_loss = 0\n",
    "        anomaly_loss = 0\n",
    "        for i in tqdm(range(len(train_data))):\n",
    "            x = train_data[i]\n",
    "            x = torch.Tensor(np.array([x])).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(x)\n",
    "            loss = objective(reconstructed, x)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            if i % 1000 == 0:\n",
    "                print(\"\\n========== \\nEpoch: \", e, \" Seq: \", i, \"\\n=============\")\n",
    "        history.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            x = torch.Tensor(np.array([test_data])).to(device)\n",
    "            reconstruction = model(x)\n",
    "            test_loss = objective(reconstruction, x)\n",
    "            test_losses.append(test_loss.item())\n",
    "            # anomaly\n",
    "            x = torch.Tensor(np.array([anomaly_norm_data])).to(device)\n",
    "            reconstruction = model(x)\n",
    "            anomaly_loss = objective(reconstruction, x)\n",
    "            anomaly_losses.append(anomaly_loss.item())\n",
    "        # if e % 10 == 0:\n",
    "        torch.save(model.state_dict(), \"res\" + os.sep + current_run_dir + os.sep + \"epoch_\" + str(e) + \".model\")\n",
    "        print(\"================================\")\n",
    "        print('Epoch: ', e);\n",
    "        print(\"Loss: \", loss.item())\n",
    "        print(\"Test Loss: \", test_loss.item())\n",
    "        print(\"Anomaly Loss: \", anomaly_loss.item())\n",
    "        print(\"================================\")\n",
    "    return history, test_losses, anomaly_losses\n",
    "\n",
    "def predict(model, dataset):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        x = torch.Tensor(np.array([dataset])).to(device)\n",
    "        reconstruction = model(x)\n",
    "    return reconstruction.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61da9a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-28fa147c365f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# CONSTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def use_pca(norm_data, DATA, n_components=0.90):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit_transform(norm_data)\n",
    "    n_pcs = pca.n_components_\n",
    "    most_important = [[np.abs(pca.components_[i]).argmax(), pca.components_[i].max()] for i in range(n_pcs)]\n",
    "    most_important = sorted(most_important , key=lambda a_entry: -a_entry[1])\n",
    "\n",
    "    two_cols = pca.components_[:2, :]\n",
    "    fig, axs = plt.subplots(1, figsize=(10, 10))\n",
    "    axs.plot(abs(two_cols.T), \".\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    initial_feature_names = DATA.columns\n",
    "    most_important_column_names = [initial_feature_names[most_important[i][0]] for i in range(n_pcs)]\n",
    "    print('Most important columns number ', len(most_important_column_names))\n",
    "    print('Most important columns', most_important_column_names)\n",
    "    NEW_DATA = norm_data[most_important_column_names].values\n",
    "    return NEW_DATA, most_important_column_names\n",
    "\n",
    "def ploy_data(norm_data, anomaly_norm_data, current_run_dir, title=\"Data\"):\n",
    "    fig, axs = plt.subplots(1, figsize=(10, 10))\n",
    "    axs.scatter(norm_data[:,0], norm_data[:,1], label=\"Normal\", color=\"g\")\n",
    "    axs.scatter(anomaly_norm_data[:,0], anomaly_norm_data[:,1], label=\"Anomaly\", color=\"r\")\n",
    "    axs.set_title(title)\n",
    "    axs.legend()\n",
    "    plt.legend()\n",
    "    plt.savefig(os.getcwd() + os.sep + 'res' + os.sep + current_run_dir + os.sep + title.replace(\"/\", \"_\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def filter_column_indexs(columns, other_columns):\n",
    "    filtered = []\n",
    "    for c in other_columns:\n",
    "        if c in columns:\n",
    "            filtered.append(columns.index(c))\n",
    "    return filtered\n",
    "\n",
    "def main():\n",
    "    current_run_dir = datetime.datetime.now().strftime(\"%d-%m-%Y_%H%M%S\")\n",
    "    os.mkdir(os.getcwd() + os.sep + 'res' + os.sep + current_run_dir)\n",
    "    DATA = pandas.read_csv('../../../rnn/merged.csv')\n",
    "    ANOMALY_DATA = pandas.read_csv('../../../rnn/merged_test.csv')\n",
    "    norm_data, columns = DATA.values, DATA.columns\n",
    "    n_features = len(columns)\n",
    "    if USE_PCA:\n",
    "        norm_data, columns = use_pca(normalize_data(DATA.copy()), DATA)\n",
    "        n_features = len(columns)\n",
    "    else:\n",
    "        norm_data, columns = normalize_data(DATA.copy()).values, list(DATA.columns.values)\n",
    "        n_features = len(DATA.columns)\n",
    "\n",
    "    anomaly_columns = ANOMALY_DATA.columns\n",
    "    # find intersected columns\n",
    "    inter_columns = np.intersect1d(columns, anomaly_columns)\n",
    "    inter_columns = [x for _, x in sorted(zip(columns, inter_columns), key=lambda pair: pair[0])]\n",
    "    anomaly_norm_data = normalize_data(ANOMALY_DATA.copy())[inter_columns].values\n",
    "    # take only data with intersected columns\n",
    "    filtered_index = filter_column_indexs(columns, inter_columns)\n",
    "    norm_data = norm_data[:, filtered_index]\n",
    "    columns = inter_columns\n",
    "\n",
    "    for i in range(len(norm_data[0]) - 2):\n",
    "        nd = norm_data[:, i:i+2]\n",
    "        a_nd = anomaly_norm_data[:, i:i+2]\n",
    "        c = ', '.join(columns[i:i+2])\n",
    "        ploy_data(nd, a_nd, current_run_dir, c)\n",
    "\n",
    "    norm_data = norm_data[:, :2]\n",
    "    columns = columns[:2]\n",
    "    anomaly_norm_data = anomaly_norm_data[:, :2]\n",
    "    n_features = len(columns)\n",
    "    print(n_features)\n",
    "    ploy_data(norm_data, anomaly_norm_data, current_run_dir, ', '.join(columns))\n",
    "\n",
    "\n",
    "    if TEST_PURPOSE:\n",
    "        norm_data = norm_data[:200]\n",
    "\n",
    "\n",
    "    # lstm_stacks = 2\n",
    "    # autoencoder_input = n_features\n",
    "    # encoder_hidden_layers = int(n_features/2)\n",
    "    # decoder_input = n_features\n",
    "    # seq_len = 60\n",
    "    # autoencoder_output = n_features\n",
    "    #\n",
    "    # autoencoder = Autoencoder(lstm_stacks,\n",
    "    #                           autoencoder_input,\n",
    "    #                           encoder_hidden_layers,\n",
    "    #                           decoder_input,\n",
    "    #                           seq_len,\n",
    "    #                           autoencoder_output).to(device)\n",
    "    # train_size = np.int(len(norm_data) * 0.8)\n",
    "    # train_data, test_data = norm_data[:train_size], norm_data[train_size:]\n",
    "    # train_x, train_y = sliding_windows(train_data, seq_len)\n",
    "    # h, l, a = train(autoencoder, train_x, test_data, anomaly_norm_data, current_run_dir)\n",
    "    #\n",
    "    # print(h)\n",
    "    # print(l)\n",
    "    # fig, axs = plt.subplots(1)\n",
    "    #\n",
    "    # axs.plot(range(len(h)), h, label=\"Train loss\")\n",
    "    # axs.plot(range(len(l)), l, label=\"Test loss\")\n",
    "    # axs.plot(range(len(a)), a, label=\"Anomaly loss\")\n",
    "    # axs.set_title('Losses')\n",
    "    # axs.legend()\n",
    "    # plt.legend()\n",
    "    # plt.savefig(os.getcwd() + os.sep + 'res' + os.sep + current_run_dir + os.sep + 'train_test')\n",
    "    # plt.show()\n",
    "    # plt.close()\n",
    "    # torch.save(autoencoder.state_dict(), \"res\" + os.sep + current_run_dir + os.sep + \"final.model\")\n",
    "    # reconstruction = predict(autoencoder, test_data)\n",
    "    # reconstruction = unnormalize_data(columns, reconstruction.cpu().numpy())\n",
    "    #\n",
    "    # original = DATA[columns].values[:len(test_data)]\n",
    "    # for i in range(len(reconstruction)):\n",
    "    #     plt.figure(figsize=(30, 10))\n",
    "    #     plt.plot(range(len(reconstruction[i])), reconstruction[i], label=\"Reconstructed \" + columns[i])\n",
    "    #     plt.plot(range(len(original[:,i])), original[:,i], label=\"Original \" + columns[i])\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "    #     plt.savefig(os.getcwd() + os.sep + 'res' + os.sep + current_run_dir + os.sep + str(i))\n",
    "    #     plt.close()\n",
    "    #\n",
    "    # file = open(os.getcwd() + os.sep + 'res' + os.sep + current_run_dir + os.sep + 'model_params.txt', \"w+\")\n",
    "    # file.writelines([\n",
    "    #     'lstm_stacks='+str(lstm_stacks),\n",
    "    #     '\\n\\rautoencoder_input='+str(autoencoder_input),\n",
    "    #     '\\n\\rencoder_hidden_layers='+str(encoder_hidden_layers),\n",
    "    #     '\\n\\rdecoder_input='+str(decoder_input),\n",
    "    #     '\\n\\rseq_len='+str(seq_len),\n",
    "    #     '\\n\\rautoencoder_output=' + str(autoencoder_output)\n",
    "    # ])\n",
    "    # file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802e2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b8714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
